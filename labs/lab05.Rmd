---
title: "Lab 5"
author: "Jia Yu Lin"
output:
  pdf_document:
    latex_engine: xelatex
date: "11:59PM March 18, 2021"
editor_options: 
  chunk_output_type: inline
---
  
  
Create a 2x2 matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns.

```{r}
#TO-DO
norm_vec = function(v){
  sqrt(sum(v^2))
}

X <- matrix(1:1, nrow=2, ncol=2)
X[,2] = rnorm(2)
 cos_theta = t(X[,1] %*% X[,2]) / (norm_vec(X[,1])*norm_vec(X[,2]))
cos_theta

#cos_theta is supposed to be near 0, but it is not near zero because it is random

abs(90 - acos(cos_theta)*180/pi)
```

Repeat this exercise `Nsim = 1e5` times and report the average absolute angle.

```{r}
#TO-DO

Nsim = 1e5
angles = array(NA,Nsim)
for( j in 1:Nsim){
  X <- matrix(1:1, nrow=2, ncol=2)
  X[,2] = rnorm(2)
  cos_theta = t(X[,1] %*% X[,2]) / (norm_vec(X[,1])*norm_vec(X[,2]))
  cos_theta

  angles[j] =  abs(90 - acos(cos_theta)*180/pi) 
}

mean(angles)
```

Create a 2xn matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns. For n = 10, 50, 100, 200, 500, 1000, report the average absolute angle over `Nsim = 1e5` simulations.

```{r}
#TO-DO
N_s = c(2,5,10,50,100,200,500,1000)
Nsim = 1e5
angles = matrix(NA,nrow = Nsim, ncol=length(N_s))
for(i in 1:length(N_s)){
  for( j in 1:Nsim){
    X <- matrix(1, nrow=N_s[i], ncol=2)
    X[,2] = rnorm(N_s[i])
    cos_theta = t(X[,1] %*% X[,2]) / (norm_vec(X[,1])*norm_vec(X[,2]))
    cos_theta
  
    angles[j,i] =  abs(90 - acos(cos_theta)*180/pi) 
  }
}

colMeans(angles)

#in 2 dimension, you are 45 degrees to 90 degrees and so on...
```

What is this absolute angle converging to? Why does this make sense?
  
#TO-DO:
The absolute angle difference from 90 is converging to zero. This makes sense because in a high dimensional space, random directions are orthogonal.
  
Create a vector y by simulating n = 100 standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the R^2 of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
#TO-DO
n = 100
X = cbind(1, rnorm(n))
y = rnorm(n)
head(X)


H = X %*% solve((t(X) %*% X)) %*% t(X)
y_hat = H %*% y
y_bar = mean(y)

SSR = sum((y_hat - y_bar)^2)
SST = sum((y- y_bar)^2)

Rsq = (SSR / SST)
Rsq

```

Write a for loop to each time bind a new column of 100 standard iid normals to the matrix X and find the R^2 each time until the number of columns is 100. Create a vector to save all R^2's. What happened??

```{r}
#TO-DO

Rsq_s = array(NA, dim=n-2)

for(j in 1:(n-2)){
  X = cbind(X, rnorm(n))
  H = X %*% solve((t(X) %*% X)) %*% t(X)
  y_hat = H %*% y
  y_bar = mean(y)
  
  SSR = sum((y_hat - y_bar)^2)
  SST = sum((y- y_bar)^2)
  
  Rsq_s[j] = (SSR / SST)

}

Rsq_s
diff(Rsq_s)
```

Test that the projection matrix onto this X is the same as I_n. You may have to vectorize the matrices in the `expect_equal` function for the test to work.

```{r}
pacman::p_load(testthat)
#TO-DO
dim(X)
H = X %*% solve((t(X) %*% X)) %*% t(X)
#H[1:10,1:10]

I = diag(n)
expect_equal(H,I)
#tolerance of test is between e-10 to e-8 (i think)
#this kind of test change will matter if you do this as a living since it can make a huge difference
#this test "was" expected to fail because of last year, but test package changed/updated
```

Add one final column to X to bring the number of columns to 101. Then try to compute R^2. What happens? 

```{r eval=FALSE}
#TO-DO
X = cbind(X, rnorm(n))
H = X %*% solve((t(X) %*% X)) %*% t(X)
#can't invert because it's rank deficient, so we get an error for the line above
y_hat = H %*% y
y_bar = mean(y)

SSR = sum((y_hat - y_bar)^2)
SST = sum((y- y_bar)^2)

Rsq = (SSR / SST)
Rsq


```

Why does this make sense?

#TO-DO:
It fails because you cannot invert a rank deficient matrix.

Write a function spec'd as follows:
  
```{r}
#' Orthogonal Projection
#'
#' Projects vector a onto v.
#'
#' @param a   the vector to project
#' @param v   the vector projected onto
#'
#' @returns   a list of two vectors, the orthogonal projection parallel to v named a_parallel, 
#'            and the orthogonal error orthogonal to v called a_perpendicular
orthogonal_projection = function(a, v){
  #TO-DO
  H = v %*% t(v) / (norm_vec(v)^2)
  a_parallel = H %*% a
  a_perpendicular = a - a_parallel
  list(a_parallel = a_parallel, a_perpendicular = a_perpendicular)
}
```

Provide predictions for each of these computations and then run them to make sure you're correct.

```{r}
orthogonal_projection(c(1,2,3,4), c(1,2,3,4))
#prediction:
orthogonal_projection(c(1, 2, 3, 4), c(0, 2, 0, -1))
#prediction:
result = orthogonal_projection(c(2, 6, 7, 3), c(1, 3, 5, 7))
t(result$a_parallel) %*% result$a_perpendicular
#prediction:
result$a_parallel + result$a_perpendicular
#prediction: should construct the original vector
result$a_parallel / c(1, 3, 5 ,7)
#prediction: percentage of the orthogonal projection --> will get some scale
```

Let's use the Boston Housing Data for the following exercises

```{r}
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
p_plus_one = ncol(X)
n = nrow(X)
head(X)
```

Using your function `orthogonal_projection` orthogonally project onto the column space of X by projecting y on each vector of X individually and adding up the projections and call the sum `yhat_naive`.

```{r}
#TO-DO
yhat_naive = rep(0,n)
for(j in 1:p_plus_one){
  yhat_naive = yhat_naive + orthogonal_projection(y,X[,j])$a_parallel
}
```

How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection.

```{r}
#TO-DO
yhat = X %*% solve(t(X) %*% X) %*% t(X) %*% y
sqrt(sum(yhat_naive^2)) / sqrt(sum(yhat^2))
```

Is this ratio expected? Why or why not?
  
#TO-DO:
It is expected to be different from 1 because yhat_naive is not y_hat. There is a bunch of double counting that is going on.
  
Convert X into V where V has the same column space as X but has orthogonal columns. You can use the function `orthogonal_projection`. This is the Gram-Schmidt orthogonalization algorithm.

```{r}
V = matrix(NA, nrow = n, ncol = p_plus_one)
V[ , 1] = X[ , 1]
#TO-DO
for(j in 2:p_plus_one){
  V[,j] = X[,j] 
  for(k in 1:(j-1)){
    V[,j] = V[,j] - orthogonal_projection(X[,j], V[,k])$a_parallel
  }
}

V[,7] %*% V[,9]
```

Convert V into Q whose columns are the same except normalized

```{r}
Q = matrix(NA, nrow = n, ncol = p_plus_one)
#TO-DO
for( j in 1:p_plus_one){
  Q[,j] = V[,j] / norm_vec(V[,j])
}

```

Verify Q^T Q is I_{p+1} i.e. Q is an orthonormal matrix.

```{r}
#TO-DO
expect_equal(t(Q) %*% Q, diag(p_plus_one))
```

Is your Q the same as what results from R's built-in QR-decomposition function?

```{r eval=FALSE}
#TO-DO
Q_from_Rs_builtin = qr.Q(qr(X))

expect_equal(Q_from_Rs_builtin, Q)
#expected to fail
```
 
Is this expected? Why did this happen?

#TO-DO
Yes, because Q and Q_from_Rs_builtin are not equal. This happens because there is infinite orthonormal basis of any column space.

Project y onto colsp[Q] and verify it is the same as the OLS fit. You may have to use the function `unname` to compare the vectors since they the entries will likely have different names.

```{r}
#TO-DO ####

projection_data = Q %*% t(Q) %*% y
#projection_data

OLS_fit = lm(y ~ Q)$fitted.values


expect_equal(unname(OLS_fit), unname(c(projection_data)))

```

Project y onto colsp[Q] one by one and verify it sums to be the projection onto the whole space.

```{r}
#TO-DO

yhat_naive = 0
for(j in 1:p_plus_one){
  yhat_naive = yhat_naive + orthogonal_projection(y,Q[,j])$a_parallel
}

expect_equal(unname(yhat), unname(yhat_naive))
```

Split the Boston Housing Data into a training set and a test set where the training set is 80% of the observations. Do so at random.

```{r}
K = 5
n_test = round(n * 1 / K)
n_train = n - n_test
#TO-DO
test_indices = sample(1:n, n_test)
train_indices = setdiff(1:n, test_indices)

X_train = X[train_indices,]
y_train = y[train_indices]
X_test = X[test_indices,]
y_test = y[test_indices]

dim(X_train)
dim(X_test)
length(y_train)
length(y_test)
```

Fit an OLS model. Find the s_e in sample and out of sample. Which one is greater? Note: we are now using s_e and not RMSE since RMSE has the n-(p + 1) in the denominator not n-1 which attempts to de-bias the error estimate by inflating the estimate when overfitting in high p. Again, we're just using `sd(e)`, the sample standard deviation of the residuals.

```{r}
#TO-DO

mod = lm ( y_train ~ .+0, data.frame(X_train))
sd(mod$residuals)

y_hat_oos = predict(mod, data.frame(X_test))

oos_residuals = y_test - y_hat_oos
sd(oos_residuals)

```

Do these two exercises `Nsim = 1000` times and find the average difference between s_e and ooss_e. 

```{r}
#TO-DO
Nsim = 1000
#diff_sum = 0
diff_vec = c()
for( count in 1:Nsim){
  K = 5
  n_test = round(n * 1 / K)
  n_train = n - n_test
  #TO-DO
  
  test_indices = sample(1:n, n_test)
  train_indices = setdiff(1:n, test_indices)
  
  X_train = X[train_indices,]
  y_train = y[train_indices]
  X_test = X[test_indices,]
  y_test = y[test_indices]
  
  dim(X_train)
  dim(X_test)
  length(y_train)
  length(y_test)
  mod = lm ( y_train ~.+0, data.frame(X_train))

  s_e = sd(mod$residuals)
  
  y_hat_oos = predict(mod, data.frame(X_test))
  oos_residuals = y_test - y_hat_oos
  ooss_e = sd(oos_residuals)
  #diff_sum = diff_sum + abs(ooss_e - s_e)
  diff_vec <- append(diff_vec, abs(ooss_e - s_e))
}
#diff_sum/1000

mean(diff_vec)
```

We'll now add random junk to the data so that `p_plus_one = n_train` and create a new data matrix `X_with_junk.`

```{r}
X_with_junk = cbind(X, matrix(rnorm(n * (n_train - p_plus_one)), nrow = n))
dim(X)
dim(X_with_junk)

```

Repeat the exercise above measuring the average s_e and ooss_e but this time record these metrics by number of features used. That is, do it for the first column of `X_with_junk` (the intercept column), then do it for the first and second columns, then the first three columns, etc until you do it for all columns of `X_with_junk`. Save these in `s_e_by_p` and `ooss_e_by_p`.


```{r}
#TO-DO
K = 5
n_test = round(n * 1 / K)
n_train = n - n_test
Nsim = 100
#diff_sum = 0
s_e_by_p = c()
ooss_e_by_p = c()
for( i in 1:ncol(X_with_junk)){
    #TO-DO
    ooss_e_array = array(NA, dim = Nsim)
    s_e_array = array(NA, dim = Nsim)
   
    
    for(count in 1:Nsim){
      test_indices = sample(1:n, n_test)
      train_indices = setdiff(1:n, test_indices)
      X_train = X_with_junk[train_indices, 1:i, drop = FALSE]
      y_train = y[train_indices]
      X_test = X_with_junk[test_indices, 1:i, drop = FALSE]
      y_test = y[test_indices]
      
      dim(X_train)
      dim(X_test)
      length(y_train)
      length(y_test)
      mod = lm ( y_train ~ .+0, data.frame(X_train))
      s_e_array[count] =  sd(mod$residuals)
      
      y_hat_oos = predict(mod, data.frame(X_test))      
      oos_residuals = y_test - y_hat_oos
      ooss_e = sd(oos_residuals)
      ooss_e_array[count] = ooss_e
    }
    
    s_e_by_p <- append(s_e_by_p, mean(s_e_array))
    ooss_e_by_p <- append(ooss_e_by_p, mean(ooss_e_array))
}

mean(s_e_by_p)
mean(ooss_e_by_p)
```

You can graph them here:

```{r}
pacman::p_load(ggplot2)
ggplot(
  rbind(
    data.frame(s_e = s_e_by_p, p = 1 : n_train, series = "in-sample"),
    data.frame(s_e = ooss_e_by_p, p = 1 : n_train, series = "out-of-sample")
  )) +
  geom_line(aes(x = p, y = s_e, col = series))
```
 
Is this shape expected? Explain.

#TO-DO
Yes, because we are increasing the number of features so overfitting is occuring as a result. In-sample error is going to 0 because it is progressively becoming a better fit for the data. The out-of-sample error is getting exponentially worse because it is overfitting and will result in a model that will give inaccurate predictions when given new data.