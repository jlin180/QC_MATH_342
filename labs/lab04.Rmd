---
title: "Lab 4"
author: "Jia Yu Lin"
output: 
  pdf_document:
    latex_engine: xelatex
date: "11:59PM March 11, 2021"
editor_options: 
  chunk_output_type: inline
---
  
  Load up the famous iris dataset. We are going to do a different prediction problem. Imagine the only input x is Species and you are trying to predict y which is Petal.Length. A reasonable prediction is the average petal length within each Species. Prove that this is the OLS model by fitting an appropriate `lm` and then using the predict function to verify.

```{r}
data(iris)
mod =lm( Petal.Length ~ Species, iris)
mod

mean(iris$Petal.Length[iris$Species == 'setosa'])
mean(iris$Petal.Length[iris$Species == 'versicolor']) 
mean(iris$Petal.Length[iris$Species == 'virginica']) 

predict(mod, data.frame(Species = c("setosa")))
predict(mod, data.frame(Species = c("versicolor")))
predict(mod, data.frame(Species = c("virginica")))

```

Construct the design matrix with an intercept, $X$, without using `model.matrix`.

```{r}
#TO-DO
X = cbind(1,iris$Species == 'versicolor', iris$Species == 'virginica')
View(X)
head(X)
```

Find the hat matrix $H$ for this regression.

```{r}
#TO-DO
H = X %*% solve(t(X) %*% X) %*% t(X)
Matrix::rankMatrix(H)
 
#head(H)
```

Verify this hat matrix is symmetric using the `expect_equal` function in the package `testthat`.

```{r}
#TO-DO
pacman::p_load(testthat)
expect_equal(H, t(H))
```

Verify this hat matrix is idempotent using the `expect_equal` function in the package `testthat`.

```{r}
#TO-DO
expect_equal(H , H%*%H)
```

Using the `diag` function, find the trace of the hat matrix.

```{r}
#TO-DO
#trace is the sum of diagonal
sum(diag(H))
#sum of trace is rank

```

It turns out the trace of a hat matrix is the same as its rank! But we don't have time to prove these interesting and useful facts..

For masters students: create a matrix $X_\perp$.

```{r}
#TO-DO

```

Using the hat matrix, compute the $\hat{y}$ vector and using the projection onto the residual space, compute the $e$ vector and verify they are orthogonal to each other.

```{r}
#TO-DO
y = iris$Petal.Length
y_hat = H %*% iris$Petal.Length
#we are supposed to see y bars for setosa, versicolor, or virignica
#table(y_hat)
I = diag(nrow(iris))
e = (I - H) %*% y
head(e)

Matrix::rankMatrix(I-H)
```

Compute SST, SSR and SSE and $R^2$ and then show that SST = SSR + SSE.

```{r}
#TO-DO
SSE = t(e) %*% e  #same thing is sum(e^2)
y_bar = mean(y)
SST = t(y - y_bar)  %*% (y - y_bar)

Rsq = 1 - SSE/SST
Rsq

SSR = t(y_hat - y_bar) %*% (y_hat - y_bar)
SSR

expect_equal(SSE+SSR, SST)
#this would mean each species would have similar petal length
```

Find the angle $\theta$ between $y$ - $\bar{y}1$ and $\hat{y} - \bar{y}1$ and then verify that its cosine squared is the same as the $R^2$ from the previous problem.

```{r}
#TO-DO
#231 formula
theta = acos(t(y - y_bar) %*% (y_hat - y_bar) / sqrt(SST * SSR))
#Rsq was pretty large so theta should be pretty small
#theta is 14 degrees
theta  * (180/pi)
cos(theta)^2
expect_equal(cos(theta)^2, Rsq)
```

Project the $y$ vector onto each column of the $X$ matrix and test if the sum of these projections is the same as yhat.

```{r eval=FALSE}
#TO-DO
proj1= (X[,1] %*% t(X[,1]) / as.numeric(t(X[,1]) %*% X[,1])) %*% y #H on to X1
proj2= (X[,2] %*% t(X[,2]) / as.numeric(t(X[,2]) %*% X[,2])) %*% y #H on to X2
proj3= (X[,3] %*% t(X[,3]) / as.numeric(t(X[,3]) %*% X[,3])) %*% y #H on to X3

expect_equal(proj1+proj2+proj3, y_hat)
#this will fail which is what we want
```

Construct the design matrix without an intercept, $X$, without using `model.matrix`.

```{r}
#TO-DO
X = cbind( 1e-4, as.numeric(iris$Species == 'versicolor'), as.numeric(iris$Species == 'virginica'))
head(X)
#iris
```

Find the OLS estimates using this design matrix. It should be the sample averages of the petal lengths within species.

```{r}
#TO-DO
mod_X = lm(Petal.Length ~ X, iris)
mod_X

```

Verify the hat matrix constructed from this design matrix is the same as the hat matrix constructed from the design matrix with the intercept. (Fact: orthogonal projection matrices are unique).

```{r}
#TO-DO
H_new = X %*% solve(t(X) %*% X) %*% t(X)

expect_equal(H_new, H, tol= 1e-4)
```

Project the $y$ vector onto each column of the $X$ matrix and test if the sum of these projections is the same as yhat.

```{r eval = FALSE}
#TO-DO
proj1= (X[,1] %*% t(X[,1]) / as.numeric(t(X[,1]) %*% X[,1])) %*% y #H on to X1
proj2= (X[,2] %*% t(X[,2]) / as.numeric(t(X[,2]) %*% X[,2])) %*% y #H on to X2
proj3= (X[,3] %*% t(X[,3]) / as.numeric(t(X[,3]) %*% X[,3])) %*% y #H on to X3

expect_equal(proj1+proj2+proj3, y_hat)

```

Convert this design matrix into $Q$, an orthonormal matrix.

```{r}
#TO-DO
qrX = qr(X)
Q = qr.Q(qrX)
```

Project the $y$ vector onto each column of the $Q$ matrix and test if the sum of these projections is the same as yhat.

```{r}
#TO-DO

proj1= (Q[,1] %*% t(Q[,1]) / as.numeric(t(Q[,1]) %*% Q[,1])) %*% y #H on to X1
proj2= (Q[,2] %*% t(Q[,2]) / as.numeric(t(Q[,2]) %*% Q[,2])) %*% y #H on to X2
proj3= (Q[,3] %*% t(Q[,3]) / as.numeric(t(Q[,3]) %*% Q[,3])) %*% y #H on to X3

expect_equal(proj1+proj2+proj3, y_hat)
```

Find the $p=3$ linear OLS estimates if $Q$ is used as the design matrix using the `lm` method. Is the OLS solution the same as the OLS solution for $X$?

```{r}
#TO-DO

mod_Q = lm(Petal.Length ~ Q, iris)
mod_Q
#It is not the same as the OLS solution for X
```

Use the predict function and ensure that the predicted values are the same for both linear models: the one created with $X$ as its design matrix and the one created with $Q$ as its design matrix.

```{r}
#TO-DO

pred_X = predict(mod_X)
pred_Q = predict(mod_Q)

expect_equal(pred_X, pred_Q)

```


Clear the workspace and load the boston housing data and extract $X$ and $y$. The dimensions are $n=506$ and $p=13$. Create a matrix that is $(p + 1) \times (p + 1)$ full of NA's. Label the columns the same columns as X. Do not label the rows. For the first row, find the OLS estimate of the $y$ regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the $y$ regressed on the first and second columns of $X$ only and put them in the first and second entries. For the third row, find the OLS estimates of the $y$ regressed on the first, second and third columns of $X$ only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates.

```{r}
#TO-DO
rm(list = ls())

boston = MASS::Boston
#?Boston
X = cbind(1,as.matrix(boston[, 1:13]))
Y = boston[, 14]


X_matrix = matrix(NA, nrow= 14, ncol = 14)

colnames(X_matrix) <- c(colnames(X))
X_matrix



for( i in 1:ncol(X)){
  mod_coef = coef(lm(Y ~ X[,1:i], data= as.data.frame(boston)))
  count = 1
  for( j in 2:(length(mod_coef))){
    X_matrix[i, count] = mod_coef[j]
    X_matrix[i, 1] = mod_coef[1]
    count = count+1
  }
}
X_matrix
```


Why are the estimates changing from row to row as you add in more predictors?
  
  #TO-DO
  The estimates are changing as more predictors are added because it is trying to find a better fit.
  
  Create a vector of length $p+1$ and compute the R^2 values for each of the above models. 

```{r}
#TO-DO
y <- c()
for( j in 1:14){
  mod_coef = lm(Y~X[,1:j])
  y <- append(y, summary(mod_coef)$r.squared)
}
y
```

Is R^2 monotonically increasing? Why?
  
  #TO-DO R square increases because the amount of features we have went up